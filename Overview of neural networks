目录

第一章
  引言
    本文动机
    本文目标
第二章
  多层网络结构
    神经网络
    循环神经网络
    卷积神经网络
    生成对抗网络
    多层网络的训练
    迁移学习
  空间卷积神经网络
    卷积神经网络的不变形
    卷积神经网络中的目标定位问题
  时域卷积神经网络
  总结
第三章
  理解卷积神经网络的构建模块
    卷积层
    非线性单元
    归一化
    池化操作   
第四章
  当前研究状态
    当前趋势
      卷积的可视化分析
      卷积的消融学习
      卷积结构的控制设计
    待解决问题
    
第一章

引言

▌本文动机

过去几年，计算机视觉研究主要集中在卷积神经网络上（通常简称为 ConvNet 或 CNN），在大量诸如分类和回归任务上已经实现了目前为止最佳的表现。
尽管这些方法的历史可以追溯到多年前，但相对而言，对这些方法的理论理解及对结果的解释还比较浅薄。实际上，计算机视觉领域的很多成果都把 CNN 
当作了一种黑箱，这种方式虽然有效的，但对结果的解释却是模糊不清的，这也无法满足科学研究的需求。尤其是当这两个问题是互补关系时：
（1）学习的方面（比如卷积核），它到底学习到的是什么？
（2）模型结构设计方面（比如卷积层数量、卷积核数量、池化策略、非线性函数的选择），为什么某些组合会优于其他组合呢？求解这些问题的答案，不
 仅有利于我们更好地理解卷积神经网络，而且还能进一步提升它的工程实用性。

此外，当前 CNN 的实现方法都需要大量训练数据，而且模型的设计方案对最终的结果有很大的影响。而更深层的理论理解应该减轻模型对数据的依赖性。
尽管大量的研究已经集中在卷积神经网络的实现方式，但目前为止，这些研究结果很大程度上还只局限在对卷积操作内部处理的可视化上，目的是为了理
解卷积神经网络中不同层的变化情况。

▌本文目标

针对以上问题，本文将综述几种当前最优秀的多层卷积结构模型。更重要的是，本文还将通过不同方法来总结标准卷积神经网络的各种组件，并介绍它们
所基于的生物学或合理的理论基础。此外，本文还将介绍如何通过可视化方法及实例研究来尝试理解卷积神经网络内部的变化情况。我们的最终目标是向
读者详细展示卷积神经网络中所涉及到的每一个卷积层操作，着重强调当前最先进的卷积神经网络模型并说明未来仍需解决的问题。

第二章



▌多层网络结构

近年来，在深度学习或深层神经网络取得成功前，计算机视觉识别系统最先进的方法主要由两个步骤组成，这两个步骤各自分离但又互补：首先，我们需要
通过人工设计操作（如卷积、局部或全局编码方法）将输入数据转换成合适的形式。这种输入的变换形式，通常是为了得到输入数据的一种紧凑或抽象的表
征，同时还要根据当前任务的需要手动设计一些不变量。通过这种转换，我们能够将输入数据表征成一种更容易分离或识别的形式，这有助于后续的识别分
类。其次，转换后的数据通常作为分类器（如支持向量机）训练的输入信号。通常而言，任何分类器的表现都会受到变换后的数据质量及所使用的变换方法
的影响。

多层神经网络结构的出现为解决这一问题带来了新的方式，这种多层结构不仅能够训练目标分类器，还能从输入数据中直接学习所需的变换操作。这种学习
方式通常称为表征学习，当将其应用在深度或多层神经网络结构中时，我们称之为深度学习。

多层神经网络定义为是一种从输入数据的层次抽象表征中提取有用信息的计算模型。一般而言，设计多层网络结构的目标是为了在高层凸显输入数据的重要
信息，同时能让那些不太不重要的信息变化更具鲁棒性。

近年来，研究者已经提出了很多不同类型的多层架构，而大多数的多层神经网络都是以堆叠的方式，将一些线性和非线性函数模块组合形成多层结构。本章
将会覆盖计算机视觉应用中最先进的多层神经网络结构。其中，人工神经网络是我们需要的关注重点，因为这种网络结构的表现非常突出。为了方便起见，
在下文我们会直接将这类网络称为神经网络。

神经网络

标准的神经网络结构通常由输入层 x，输出层 y 和多个隐藏层 h 堆叠而成，其中每个层还由多个单元组成，如下图所示。通常，每个隐藏单元 hj 接受
上一层所有单元的输入，并将其加权组合，其非线性组合的数学形式如下：



wij  是权重值，用于控制输入单位和隐藏单位之间连接的强度，bj 是隐藏单位的偏置，F 是非线性函数，如 Sigmoid 函数。

深度神经网络可以被视为是 Rosenblatt 感知器及多层感知器的实例。 尽管神经网络模型已经存在多年（即自 1960 年代以来），但它们并未被广泛使用。造成这种的原因有很多，最主要的原因是感知器无法模拟像 XOR 这样的简单操作而被外界否定，这也进一步阻碍了研究人员对感知器的研究。

直到最近，一些研究人员将简单感知器扩展到多层神经网络模型。 此外，缺乏适当的训练算法也会延缓感知度的训练进度，而反向传播算法的提出也使得神经网络模型得以普及。更重要的是，多层神经网络结构依赖于大量的参数，这就意味着我们需要大量的训练数据和计算资源来支持模型训练及学习参数过程。


﻿﻿标准神经网络结构示意图
 
受限波尔茨曼机（RBM）的提出是深层神经网络领域的一大重要贡献。受限玻耳兹曼机可以看作是两层的神经网络，只允许网络以前馈连接的方式堆叠。而神经网络可以看作是使用受限波尔茨曼机进行分层无监督预训练的一种模型，在图像识别任务中，这种无监督学习方法主要包括三个步骤：首先，对于图像中的每个像素，对 xi 及初始化的 wij、偏置 bj、隐藏层状态 hj，其概率可以被定义为：
﻿

﻿
其中，σ（y）= 1 /（1 + exp（-y））。

其次，如上式所示，一旦所有的隐藏状态都被随机设定，我们可以根据概率﻿将每个像素设定为 1，并以此重建图像。

然后，隐藏单元将通过重建的权重和偏差来更新校正单位的误差：
﻿


其中，α 是学习率，（xihj）表示隐藏单元 hj 中像素 xi 出现的次数。整个训练过程将重复 N 次或直到误差下降到预设的阈值 τ。训练完一层后，使用它的输出作为下一层的输入，然后接着重复上述过程训练下一层。通常，网络中的所有层经过预训练后，它们还将通过梯度下降的方式，反向传播误差来进一步微调标记数据。使用这种分层无监督预训练的方式可以不需大量标记数据的情况下，训练深层神经网络结构。因为利用受限波尔茨曼机进行无监督预训练，能够为模型参数的初始化提供了一种有效途径。受限波尔茨曼机的第一个成功应用案例是用于人脸识别的降维，它们被当作是一种自编码器。

自动编码器主要是通过引入不同的正则化方法来防止模型学习一些无关紧要的数据特征。目前一些比较优秀的编码器包括稀疏自编码器、去噪自编码器（DAE）和压缩自编码器（CAE）等。稀疏自编码器允许中间编码表示的大小（即由输入生成编码器）大于输入的大小，同时通过稀疏表示来正则化负相的输出。相反，去噪自编码器改变了编码重建本身的目标，试图重建一个干净、不带噪声的输入版本，得到一个更加强大的表示。类似地，压缩自编码器是通过惩罚噪声中最敏感的单位来实现类似去噪自编码器的过程。

﻿﻿标准的自编码器结构
 
循环神经网络

循环神经网络是处理序列数据相关任务最成功的多层神经网络模型（RNN）。 RNN，其结构示意图如下图所示，它可以看作是神经网络的一种特殊类型，隐藏单元的输入由当前时间步所观察到的数据中获取输入以及它在前一个时间步的状态组合而成。 循环神经网络的输出定义如下：



其中 σ 表示一些非线性函数，wi 和ui 是网络参数，用于控制当前和过去信息的相对重要性。

﻿﻿标准的循环神经网络结构示意图

每个循环单元的输入将由当前时刻的输入 xt 及上一时刻 ht-1 组成，新的输出表示可通过上式计算得到，并传递给循环神经网络中的其他层。

虽然循环神经网络是一类强大的多层神经网络模型，但其的主要问题是模型对时间的长期依赖性，由于梯度爆炸或梯度消失，这种限制将导致模型训练过程在网络回传过程中误差的不平稳变化。为了纠正这个困难，引入了长短期记忆网络（LSTM）。

长短期记忆网络（LSTM）的结构示意图下图所示，拥有存储单元或记忆单元，随着时间的推移存储记忆信息。LSTM 的存储单元是通过门控机制从中读取信息或写入信息。 值得注意的是，LSTM 还包含遗忘门，即网络能够删除一些不必要的信息。总的来说， LSTM 的结构主要包含有：三个控制不同的门（输入门、遗忘门及输出门），以及存储单元状态。 输入门由当前输入 xt 和前一个状态 ht-1 控制，它的定义如下：
﻿


其中，wi,ui,bi 表示权重和偏差项，用于控制与输入门相关的权重，σ 通常是一个 Sigmoid 函数。类似地，遗忘门定义如下：



相应地，权重和偏差项由 wf,uf,bf 控制。 可以说，LSTM 最重要的一点是它可以应对梯度消失或梯度爆炸时网络中误差传播不平稳的挑战。这种能力的实现是通过遗忘门和输入门的状态进行加法结合来确定存储单元的状态。

﻿﻿标准的长短期记忆网络结构示意图

每个循环单元的输入将由当前时刻的输入 xt 及上一时刻 ht-1 组成，网络的返回值将馈送到下一时刻 ht。LSTM 最终的输出由输入门 it，遗忘门 ft 及输出门 ot 和记忆单元状态 ct 共同决定。
